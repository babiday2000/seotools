import { jsx as _jsx, jsxs as _jsxs } from "react/jsx-runtime";
import { useState } from 'react';
import { Textarea } from '@/components/ui/textarea';
import { Button } from '@/components/ui/button';
import { Clipboard, ClipboardCheck } from 'lucide-react';
const RobotsTxtGeneratorTool = () => {
    const [rules, setRules] = useState('');
    const [generatedTxt, setGeneratedTxt] = useState('');
    const [copied, setCopied] = useState(false);
    const handleGenerate = () => {
        // This is a placeholder for the actual generation logic.
        // In a real application, this would involve more complex rule generation.
        const generated = `User-agent: *\n${rules}`;
        setGeneratedTxt(generated);
    };
    const handleCopy = () => {
        navigator.clipboard.writeText(generatedTxt);
        setCopied(true);
        setTimeout(() => setCopied(false), 2000);
    };
    return (_jsxs("div", { className: "space-y-4", children: [_jsxs("div", { className: "grid grid-cols-1 gap-4 md:grid-cols-2", children: [_jsx(Textarea, { placeholder: "Enter your rules here, one per line (e.g., Disallow: /private/)", className: "h-64 text-base", value: rules, onChange: (e) => setRules(e.target.value) }), _jsxs("div", { className: "relative", children: [_jsx(Textarea, { placeholder: "Generated robots.txt will appear here...", className: "h-64 text-base", value: generatedTxt, readOnly: true }), _jsx(Button, { variant: "ghost", size: "icon", className: "absolute top-2 right-2", onClick: handleCopy, disabled: !generatedTxt, children: copied ? _jsx(ClipboardCheck, { className: "h-5 w-5 text-green-500" }) : _jsx(Clipboard, { className: "h-5 w-5" }) })] })] }), _jsx(Button, { onClick: handleGenerate, disabled: !rules, children: "Generate robots.txt" }), _jsxs("div", { className: "max-w-4xl mx-auto space-y-12 text-left", children: [_jsxs("section", { children: [_jsx("h2", { className: "text-2xl font-bold tracking-tight", children: "What is a Robots.txt File?" }), _jsxs("div", { className: "mt-4 space-y-4 text-muted-foreground", children: [_jsx("p", { children: "A `robots.txt` file is a simple text file that resides in the root directory of a website. Its primary purpose is to communicate with web crawlers, also known as bots or spiders, which are automated scripts used by search engines like Google, Bing, and others to index the web. This file instructs these crawlers on which pages or sections of a website should not be crawled or indexed. It's a fundamental part of the Robots Exclusion Protocol (REP), a standard that provides a way for webmasters to control how their sites are accessed by automated clients." }), _jsx("p", { children: "Think of it as a doorman for your website, guiding friendly bots to the right places and keeping them out of areas you'd prefer to remain private or unindexed. While it's not a security measure\u2014malicious bots will likely ignore it\u2014it's an essential tool for search engine optimization (SEO) and for managing how your content appears in search results. By effectively managing crawl traffic, you can ensure that search engines prioritize the most important content on your site, leading to better crawl efficiency and potentially improved rankings." })] })] }), _jsxs("section", { children: [_jsx("h2", { className: "text-2xl font-bold tracking-tight", children: "Factors to Consider When Creating a Robots.txt File" }), _jsxs("div", { className: "mt-4 space-y-4 text-muted-foreground", children: [_jsx("p", { children: "Creating a `robots.txt` file isn't just about listing a few directories to disallow. A thoughtful approach considers several factors to maximize its benefits for SEO and site management:" }), _jsxs("ul", { className: "list-disc list-inside space-y-3 pl-4", children: [_jsxs("li", { children: [_jsx("strong", { children: "Website Structure:" }), " Understand your site's architecture. Identify directories that contain sensitive information, admin pages, scripts, or duplicate content that shouldn't be indexed."] }), _jsxs("li", { children: [_jsx("strong", { children: "Crawl Budget:" }), " For large websites, crawl budget\u2014the number of pages a search engine bot will crawl on any given day\u2014is a critical concern. A well-configured `robots.txt` can guide bots to your most valuable pages, ensuring your crawl budget is spent efficiently."] }), _jsxs("li", { children: [_jsx("strong", { children: "SEO Strategy:" }), " Your `robots.txt` file should align with your overall SEO goals. For instance, you might want to block faceted navigation URLs that create duplicate content, or prevent indexing of internal search result pages."] }), _jsxs("li", { children: [_jsx("strong", { children: "Sitemaps:" }), " Including a link to your XML sitemap in the `robots.txt` file is a best practice. It helps search engines discover all the important pages on your site, even if they are not well-linked internally."] }), _jsxs("li", { children: [_jsx("strong", { children: "Subdomains:" }), " If your site uses subdomains, remember that each subdomain requires its own `robots.txt` file. The rules in a `robots.txt` file on `www.example.com` will not apply to `blog.example.com`."] })] })] })] }), _jsxs("section", { children: [_jsx("h2", { className: "text-2xl font-bold tracking-tight", children: "Key Components of a Robots.txt File" }), _jsxs("div", { className: "mt-4 space-y-4 text-muted-foreground", children: [_jsx("p", { children: "A `robots.txt` file is composed of directives, which are commands that the web crawlers follow. The basic syntax is straightforward, consisting of a user-agent and one or more rules:" }), _jsxs("ul", { className: "list-disc list-inside space-y-3 pl-4", children: [_jsxs("li", { children: [_jsx("strong", { children: "User-agent:" }), " This directive specifies the bot to which the rules apply. For example, `User-agent: Googlebot` targets Google's main crawler. You can use an asterisk (`User-agent: *`) to apply the rules to all bots."] }), _jsxs("li", { children: [_jsx("strong", { children: "Disallow:" }), " This is the most common directive. It tells the specified user-agent not to crawl a particular URL path. For example, `Disallow: /admin/` will block all bots from crawling the content in the `/admin/` directory."] }), _jsxs("li", { children: [_jsx("strong", { children: "Allow:" }), " This directive, supported by major search engines like Google, allows you to permit crawling of a specific URL within a disallowed directory. For instance, if you've disallowed `/media/`, but want to allow access to one file, you could use `Allow: /media/press-release.pdf`."] }), _jsxs("li", { children: [_jsx("strong", { children: "Sitemap:" }), " This directive points to the location of your XML sitemap. It's an effective way to ensure search engines can find your sitemap. Example: `Sitemap: https://www.example.com/sitemap.xml`."] }), _jsxs("li", { children: [_jsx("strong", { children: "Crawl-delay:" }), " This directive specifies a waiting period in seconds between successive crawl requests to your server. It can be useful for preventing server overload on sites with heavy crawl traffic. However, Googlebot does not follow this directive; for Google, you should set your crawl rate in Google Search Console."] })] })] })] }), _jsxs("section", { children: [_jsx("h2", { className: "text-2xl font-bold tracking-tight", children: "How to Use Our Robots.txt Generator" }), _jsxs("div", { className: "mt-4 space-y-4 text-muted-foreground", children: [_jsx("p", { children: "Our tool simplifies the process of creating a `robots.txt` file. Follow these steps to generate a custom file for your website:" }), _jsxs("ol", { className: "list-decimal list-inside space-y-3 pl-4", children: [_jsxs("li", { children: [_jsx("strong", { children: "Specify Rules:" }), " In the input area, enter the directives you want to include. For each rule, specify the user-agent and the paths to allow or disallow. You can add multiple rules for different bots."] }), _jsxs("li", { children: [_jsx("strong", { children: "Generate the File:" }), " Click the \"Generate robots.txt\" button. The tool will compile your rules into a correctly formatted `robots.txt` file in the output area."] }), _jsxs("li", { children: [_jsx("strong", { children: "Review and Customize:" }), " Carefully examine the generated text. Ensure it accurately reflects your intentions. You can manually edit the output if needed."] }), _jsxs("li", { children: [_jsx("strong", { children: "Test Your File:" }), " Before deploying, it's crucial to test your `robots.txt` file. Use a tool like Google's robots.txt Tester (available in Google Search Console) to verify that your rules work as expected and don't block important content."] }), _jsxs("li", { children: [_jsx("strong", { children: "Deploy to Your Website:" }), " Once you're confident in your `robots.txt` file, upload it to the root directory of your domain. It must be accessible at `https://www.yourdomain.com/robots.txt`."] })] })] })] }), _jsxs("section", { children: [_jsx("h2", { className: "text-2xl font-bold tracking-tight", children: "Frequently Asked Questions (FAQ)" }), _jsxs("div", { className: "mt-4 space-y-6", children: [_jsxs("div", { children: [_jsx("h3", { className: "font-semibold", children: "What happens if I don't have a `robots.txt` file?" }), _jsx("p", { className: "text-muted-foreground mt-1", children: "If a website does not have a `robots.txt` file, search engine crawlers will assume they have permission to crawl the entire site. This can be problematic if you have pages that you do not want to appear in search results." })] }), _jsxs("div", { children: [_jsx("h3", { className: "font-semibold", children: "Can `robots.txt` be used to hide a page from Google?" }), _jsx("p", { className: "text-muted-foreground mt-1", children: "While `robots.txt` can prevent a page from being crawled, it does not guarantee that it will not be indexed. If other pages on the web link to the blocked page, Google may still index it without visiting it. To reliably prevent a page from appearing in search results, you should use the `noindex` meta tag or the `X-Robots-Tag` HTTP header." })] }), _jsxs("div", { children: [_jsx("h3", { className: "font-semibold", children: "How do I block all bots from my site?" }), _jsx("p", { className: "text-muted-foreground mt-1", children: "To block all compliant bots, you can use the following `robots.txt` file:" }), _jsx("pre", { className: "p-4 bg-gray-800 text-white rounded-md text-sm mt-2", children: _jsxs("code", { children: ["User-agent: *", _jsx("br", {}), "Disallow: /"] }) })] }), _jsxs("div", { children: [_jsx("h3", { className: "font-semibold", children: "What is the difference between `Disallow: /` and `Disallow:`?" }), _jsx("p", { className: "text-muted-foreground mt-1", children: "`Disallow: /` blocks all pages on your site. `Disallow:` (with nothing after it) means nothing is disallowed, so crawlers are free to access everything. It's equivalent to not having a `Disallow` directive at all." })] })] })] }), _jsxs("section", { children: [_jsx("h2", { className: "text-2xl font-bold tracking-tight", children: "Fun Fact" }), _jsx("div", { className: "mt-4 space-y-4 text-muted-foreground", children: _jsx("p", { children: "The `robots.txt` standard was born out of a real-world problem in the early days of the web. In 1994, Martijn Koster, a webmaster, noticed that his server was being overwhelmed by aggressive web crawlers. To solve this, he proposed the Robots Exclusion Protocol, which quickly became a de facto standard. It's a great example of the web community coming together to solve a common problem." }) })] })] })] }));
};
export default RobotsTxtGeneratorTool;
